
#  Transformer From Scratch: "Attention Is All You Need"

An educational PyTorch implementation of the classic **"Attention Is All You Need"** paper (Vaswani et al., 2017).  
This project builds the Transformer architecture **from scratch** ‚Äî covering data preprocessing, tokenization, model definition, training loop, and validation ‚Äî without relying on high-level libraries like `transformers`.

---

## ‚úèÔ∏è **Project Overview**
This implementation includes:
-  Custom Transformer encoder‚Äìdecoder
-  Tokenization using `tokenizers` library (WordLevel)
-  Dataset preparation for bilingual translation (using `opus_books` dataset)
-  Training loop with cross-entropy loss & label smoothing
-  Greedy decoding for inference / validation
-  Logging with TensorBoard
-  Model checkpointing

---

---

## üì¶ **Installation**
1. Clone the repository:
```bash
git clone https://github.com/yourusername/transformer-from-scratch.git
cd transformer-from-scratch

python train.py





